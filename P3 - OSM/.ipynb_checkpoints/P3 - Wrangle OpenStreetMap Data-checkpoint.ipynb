{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle OpenStreetMap Data: Brisbane, Australia\n",
    "## Map Area\n",
    "I chose the area for the city of Brisbane, Australia for this project. Brisbane is the third largest city in Australia and is the capital of the state of Queensland.  Brisbane is where I live, so is of specific interest to me.  \n",
    "\n",
    "A link to Brisbane in OpenStreetMaps is here: https://www.openstreetmap.org/node/1546628274#map=11/-27.4693/153.0238. \n",
    "\n",
    "I used MapZen to download the city data from this link: https://s3.amazonaws.com/metro-extracts.mapzen.com/brisbane_australia.osm.bz2\n",
    "\n",
    "## Data Auditing\n",
    "Auditing was performed using the `data.py` module included in the submission. This was largely based on the code used in Lesson 6 which shaped the data into suitable JSON documents for loading into MongoDB.  \n",
    "\n",
    "The 'blueprint' followed was to iteratively process the data using `data.py`, load the JSON data into MongoDB then perform queries to assess the quality. If issues were observed, changes were made to `data.py` and the process re-run. To begin, I used a cut-down sample of the OSM data file obtained by running `create_sample.py` which was based on the code provided in the Instructor Notes. This file is also included in the submission.\n",
    "\n",
    "What follows is a description of data issues which were encountered during this process.\n",
    "\n",
    "### Postcodes\n",
    "All Brisbane postcodes take the form of 4 digit integers starting with 4. So I chose to convert this field to an integer. Before performing this conversion I checked the assumption about the length of the postcode:\n",
    "\n",
    "```\n",
    "result = db.osm.find({'address.postcode': {'$exists': True}, '$where': 'this.address.postcode.length > 4'}, \n",
    "                     {'address.postcode': 1})\n",
    "pprint.pprint([r for r in result])\n",
    "[{u'_id': ObjectId('566d14e8b0391f9c6a1e2d6c'),\n",
    "  u'address': {u'postcode': u'QLD 4032'}}]\n",
    "```\n",
    "\n",
    "There was a single postcode in an incorrect format which included the state abbreviation in the string. I resolved this issue with a regular expression to simlpy extract the integer portion of the string. The following query now shows the minimum and maximum postcodes which are in the expected range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'postcode', u'max': 4520, u'min': 4000}]\n"
     ]
    }
   ],
   "source": [
    "import query\n",
    "db = query.get_db('test')\n",
    "\n",
    "query.aggregate(db, [{'$group': {'_id': 'postcode', \n",
    "                                 'min': {'$min': '$address.postcode'},\n",
    "                                 'max': {'$max': '$address.postcode'}}}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Street types\n",
    "I checked the street types as we did in Lesson 6. However, a different approach was needed rather than just extracting the last word of the street field. This is because streets in Brisbane can be of the form 'Stanley Street East' as an example. A less efficient method was therefore employeed which checked each word in the street address against our expected set.\n",
    "\n",
    "There were a few streets that were missing the type altogether. Fortunately, a quick Google search was able to determine what they should have been. These were added to the mapping logic in `data.py`. There was also instances of the abbreviations 'St', 'Ave' and 'Cnr' being used.\n",
    "\n",
    "The following query confirms that after auditing there are no values in the `address.street` field without at least one of the expected names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \n",
    "            \"Lane\", \"Road\", \"Trail\", \"Parkway\", \"Commons\", \"Close\", \"Parade\", \"Quay\",\n",
    "            \"Terrace\", \"Crescent\", \"Circuit\", \"Corso\", \"Highway\", \"Way\", \"Vista\",\n",
    "            \"Esplanade\", \"Grove\", \"Arterial\", \"Corner\"]\n",
    "\n",
    "len(db.osm.distinct('address.street', {'address.street': {'$regex': '^((?!' + '|'.join(expected) + ').)*$'}}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Address types\n",
    "There are a couple of address types in the data set that did not conform to our expectations. These are:\n",
    "```\n",
    "    k=address v=280 Given tce, Paddington, QLD\n",
    "    k=addr:housenumber:source v=survey\n",
    "```\n",
    "\n",
    "The first could be corrected and the second is legitimate but would require further development to handle.\n",
    "\n",
    "### Tags\n",
    "After doing some more preliminary queries to get a sense of the data, I came to the conclusion, I do not like the way the *tag* elements have been represented in the schema used in Lesson 6. Making each key a field makes it difficult to determine the schema of the data and hence perform queries. So I modified `data.py` so documents contain a *tags* field which contains arrays of key/value pairs.\n",
    "\n",
    "Despite the fact that the keys are now represented as strings rather than being a field in the document, I checked them for valid characters as we did in Lesson 6. There were a small number of 'bad' keys including:\n",
    "``` \n",
    "    Sports center\n",
    "    building.source:levels\n",
    "```\n",
    "\n",
    "These were fixed with simple string replaces.\n",
    "\n",
    "## Data Overview\n",
    "The original OSM file was 213MB. After cleaning and transforming the JSON format file has a size of 285MB and was loaded into a MongoDB database using the `mongoimport` utility:\n",
    "\n",
    "```\n",
    "C:\\Program Files\\MongoDB\\Server\\3.0\\bin>mongoimport --db test --collection osm -\n",
    "-drop --file \"C:\\Users\\Cameron\\Udacity\\P3 - OSM\\brisbane_australia.osm.json\"\n",
    "2015-12-14T00:29:43.026+1000    connected to: localhost\n",
    "2015-12-14T00:29:43.028+1000    dropping: test.osm\n",
    "2015-12-14T00:29:46.021+1000    [#.......................] test.osm     20.6 MB/\n",
    "284.6 MB (7.2%)\n",
    "2015-12-14T00:29:49.020+1000    [###.....................] test.osm     43.0 MB/\n",
    "284.6 MB (15.1%)\n",
    "2015-12-14T00:29:52.020+1000    [#####...................] test.osm     65.9 MB/\n",
    "284.6 MB (23.2%)\n",
    "2015-12-14T00:29:55.020+1000    [#######.................] test.osm     83.9 MB/\n",
    "284.6 MB (29.5%)\n",
    "2015-12-14T00:29:58.020+1000    [########................] test.osm     104.6 MB\n",
    "/284.6 MB (36.8%)\n",
    "2015-12-14T00:30:01.020+1000    [##########..............] test.osm     125.1 MB\n",
    "/284.6 MB (44.0%)\n",
    "2015-12-14T00:30:04.020+1000    [############............] test.osm     147.8 MB\n",
    "/284.6 MB (51.9%)\n",
    "2015-12-14T00:30:07.021+1000    [##############..........] test.osm     166.8 MB\n",
    "/284.6 MB (58.6%)\n",
    "2015-12-14T00:30:10.022+1000    [###############.........] test.osm     188.4 MB\n",
    "/284.6 MB (66.2%)\n",
    "2015-12-14T00:30:13.020+1000    [#################.......] test.osm     210.2 MB\n",
    "/284.6 MB (73.9%)\n",
    "2015-12-14T00:30:16.020+1000    [###################.....] test.osm     234.0 MB\n",
    "/284.6 MB (82.2%)\n",
    "2015-12-14T00:30:19.020+1000    [#####################...] test.osm     257.4 MB\n",
    "/284.6 MB (90.4%)\n",
    "2015-12-14T00:30:22.020+1000    [#######################.] test.osm     279.0 MB\n",
    "/284.6 MB (98.0%)\n",
    "2015-12-14T00:30:22.846+1000    imported 1129202 documents\n",
    "```\n",
    "\n",
    "Some basic statistics where calculated by running queries on the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1129202"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total count of records\n",
    "db.osm.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'node', u'count': 988429}, {u'_id': u'way', u'count': 140773}]\n"
     ]
    }
   ],
   "source": [
    "# Number of nodes and ways\n",
    "query.aggregate(db, [{'$group': {'_id': '$elem', 'count': {'$sum': 1}}},\n",
    "                     {'$sort': {'count': -1}}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1022"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of unique users\n",
    "len(db.osm.distinct('created.user'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'nearmop', u'count': 158738},\n",
      " {u'_id': u'morb_au', u'count': 95047},\n",
      " {u'_id': u'Unusual User Name', u'count': 87013},\n",
      " {u'_id': u'John Sinclair', u'count': 82257},\n",
      " {u'_id': u'nevw', u'count': 75957},\n",
      " {u'_id': u'David Dean', u'count': 55954},\n",
      " {u'_id': u'Peter W34', u'count': 46667},\n",
      " {u'_id': u'chas678', u'count': 42927},\n",
      " {u'_id': u'AshKyd', u'count': 37896},\n",
      " {u'_id': u'DancingFool', u'count': 29541}]\n"
     ]
    }
   ],
   "source": [
    "# Top 10 contributing users\n",
    "query.aggregate(db, [{'$group': {'_id': '$created.user', 'count': {'$sum': 1}}},\n",
    "                     {'$sort': {'count': -1}},\n",
    "                     {'$limit': 10}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'highway', u'count': 115342},\n",
      " {u'_id': u'name', u'count': 71707},\n",
      " {u'_id': u'source', u'count': 35815},\n",
      " {u'_id': u'building', u'count': 25486},\n",
      " {u'_id': u'surface', u'count': 17375},\n",
      " {u'_id': u'oneway', u'count': 16273},\n",
      " {u'_id': u'amenity', u'count': 11529},\n",
      " {u'_id': u'gtfs_id', u'count': 8326},\n",
      " {u'_id': u'public_transport', u'count': 8240},\n",
      " {u'_id': u'bus', u'count': 7851}]\n"
     ]
    }
   ],
   "source": [
    "# Top 10 tag keys\n",
    "query.aggregate(db, [{'$unwind': '$tags'},\n",
    "                     {'$group': {'_id': '$tags.key', 'count': {'$sum': 1}}},\n",
    "                     {'$sort': {'count': -1}},\n",
    "                     {'$limit': 10}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'parking', u'count': 2634},\n",
      " {u'_id': u'bench', u'count': 991},\n",
      " {u'_id': u'drinking_water', u'count': 881},\n",
      " {u'_id': u'shelter', u'count': 784},\n",
      " {u'_id': u'post_box', u'count': 540},\n",
      " {u'_id': u'school', u'count': 512},\n",
      " {u'_id': u'telephone', u'count': 480},\n",
      " {u'_id': u'fast_food', u'count': 473},\n",
      " {u'_id': u'restaurant', u'count': 434},\n",
      " {u'_id': u'toilets', u'count': 421}]\n"
     ]
    }
   ],
   "source": [
    "# Top 10 amenities\n",
    "query.aggregate(db, [{'$unwind': '$tags'},\n",
    "                     {'$match': {'tags.key': 'amenity'}},\n",
    "                     {'$group': {'_id': '$tags.value', 'count': {'$sum': 1}}},\n",
    "                     {'$sort': {'count': -1}},\n",
    "                     {'$limit': 10}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was surprised to see such a large number of schools. Perhaps this isn't a unique count. Let's verify with a different query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "515"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = db.osm.aggregate([{'$match': {'tags.value': 'school'}},\n",
    "                           {'$unwind': '$tags'},\n",
    "                           {'$match': {'tags.key': 'name'}},\n",
    "                           {'$group': {'_id': '$tags.value'}}])\n",
    "len([r for r in result])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even more! Some schools musn't have been listed as amenities. Let's see how many are within 10km of the city centre:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note 6378.1 is the radius of the Earth in km.\n",
    "db.osm.find({'tags.value': 'school', \n",
    "             'pos': {'$geoWithin': {'$centerSphere': [[153.0238, -27.4693], 10/6378.1]}}}).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about 20km?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.osm.find({'tags.value': 'school', \n",
    "             'pos': {'$geoWithin': {'$centerSphere': [[153.0238, -27.4693], 20/6378.1]}}}).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would indicate that there are a large number of schools in the outer suburbs of the city. Finally, let's check the bounds of the positions in this map area. I know that Brisbane is roughly within the grid given in the following query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.osm.find({'pos': {'$exists': True,\n",
    "                     '$not': {'$geoWithin': {'$box': [[152.4, -28.1],[153.6, -26.8]]}}\n",
    "                    }\n",
    "            }).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Futher Ideas\n",
    "### Validating point coordinates\n",
    "As you can see from some of the latter queries, I encoded the positions as GeoJSON objects. These are quite powerful as they allow execution of geospatial queries. I had hoped to be able to verify that the coordinates of each node/way were actually in the bounds of the city region. However, I was unable to source the goegraphical data so was only able to use a rough box.\n",
    "\n",
    "### Integrating data from local council\n",
    "I did manage to find a website maintained by the local council which contains a number of different spatial datasets for the city here: https://www.data.brisbane.qld.gov.au/data/group/spatial-data\n",
    "\n",
    "I have investigated a few of these datasets and I believe they would be quite straight foward to integrate. The park facilities and assets dataset for example contains over 100,000 records of park location and asset information with latitude and longitude values. This could be used to both verify and supplement existing park data for the city.\n",
    "\n",
    "### Where to live\n",
    "An idea for some research is based on a great article by Nate Silver which I discovered when I lived in NYC: http://nymag.com/realestate/neighborhoods/2010/65374/. This data driven article ranks neighbourhoods based on different characteristics such as proximity to schools, parks and crime statistics. There is even an interactive page which allows the reader to adjust the importance of these factors to give different rankings: http://nymag.com/realestate/neighborhoods/2010/65355/.\n",
    "\n",
    "It would be interesting to see if such a ranking could be produced for Brisbane (or another city) using the data contained in the OSM dataset. The primary additional data which would be required is coordinates of suburb boundaries. Then, geospatial queries such as those I've experimented with could be used to calculate the density of various tags of interest for each suburb. The user could provide a weight for each tag and a score given to the suburbs.\n",
    "\n",
    "I'm surprised at how difficult it seems to be to get suburb coordinates. Probably the best source of this info is the Australian Bureau of Statistics or Google Maps. The ABS provides such data in ESRI Shapefiles or Mapinfo Interchange formats.  I'm not familiar with either of these, so some effort and research would be required to parse these files.\n",
    "\n",
    "Searching for a suburb in Google Maps does return a shaded polygon but how (and if) this can be extracted through the maps API I'm not sure. So further research would be required here also.\n",
    "\n",
    "In the absence of this data, you could simply divide the city into a grid and perform the analysis on these regions. This would still provide useful results, but perhaps would not be as 'digestible' as if it were reported with suburb names.\n",
    "\n",
    "Other difficulties which may arise include further standardisation of the tags and timeliness of the data. In order to use rankings such as these to inform you on where to buy a house for example, up to date data would be preferable.\n",
    "\n",
    "## References\n",
    "1. https://help.github.com/articles/markdown-basics/\n",
    "2. http://wiki.openstreetmap.org/wiki/\n",
    "3. https://docs.mongodb.org/manual/tutorial/install-mongodb-on-windows/\n",
    "4. http://www.postcodes-australia.com/\n",
    "5. https://docs.python.org/2/library/re.html#regular-expression-syntax\n",
    "6. https://docs.mongodb.org/v3.0/reference/operator/query-geospatial/\n",
    "7. https://www.google.com.au/maps/place/Brisbane+QLD/@-27.4391031,151.7710868,7.46z/data=!4m2!3m1!1s0x6b91579aac93d233:0x402a35af3deaf40\n",
    "8. https://docs.mongodb.org/v3.0/reference/operator/query/regex/\n",
    "9. http://stackoverflow.com/questions/406230/regular-expression-to-match-line-that-doesnt-contain-a-word\n",
    "10. https://docs.mongodb.org/v3.0/tutorial/calculate-distances-using-spherical-geometry-with-2d-geospatial-indexes/\n",
    "11. http://www.abs.gov.au/AUSSTATS/abs@.nsf/DetailsPage/1270.0.55.003July%202011\n",
    "12. https://developers.google.com/maps/documentation/javascript/geocoding#GeocodingResults"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
